{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp\n",
        "\n",
        "!mkdir build\n",
        "%cd build\n",
        "\n",
        "!cmake ..\n",
        "!make -j"
      ],
      "metadata": {
        "id": "BBwXhDH0YNBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTByYUyGJsXH",
        "outputId": "b04159c0-4f8e-4416-8832-c696e37fae58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 70519, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 70519 (delta 87), reused 37 (delta 37), pack-reused 70357 (from 3)\u001b[K\n",
            "Receiving objects: 100% (70519/70519), 220.71 MiB | 22.63 MiB/s, done.\n",
            "Resolving deltas: 100% (50865/50865), done.\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,196 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,571 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,081 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,841 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,506 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,901 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,281 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
            "Fetched 37.8 MB in 4s (8,842 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 65 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential cmake git\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJkSjz-mQtJ7",
        "outputId": "8a0db6a8-1df2-469f-f6cb-95363cbe99e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'llama.cpp/build/bin': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls llama.cpp/build/bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKuxqGYQRbtj",
        "outputId": "694415a5-3c7d-4d98-90c1-d3daa605eba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKTpjEbkRjOL",
        "outputId": "04eda734-5448-478f-c994-23c9508b7a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "main: build = 7261 (dea9ba27c)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/drive/MyDrive/model_f16.gguf' to '/content/drive/MyDrive/model_q4_k_m.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 255 tensors from /content/drive/MyDrive/model_f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 3.2B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  27:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "llama_model_quantize_impl: n_layer_attn = 28, n_layer_recr = 0, pruned_attention_w = 0\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "llama_model_quantize_impl: model size  =  6128.17 MiB\n",
            "llama_model_quantize_impl: quant size  =  1918.35 MiB\n",
            "\n",
            "main: quantize time = 461033.76 ms\n",
            "main:    total time = 461033.76 ms\n"
          ]
        }
      ],
      "source": [
        "# !./llama.cpp/build/bin/llama-quantize \\\n",
        "#     /content/drive/MyDrive/model_f16.gguf \\\n",
        "#     /content/drive/MyDrive/model_q4_k_m.gguf \\\n",
        "#     q4_k_m\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade llama-cpp-python\n",
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGLgtDYpGpyZ",
        "outputId": "070e8e59-273a-45b5-f6a3-a8c60679a80e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4422320 sha256=4b9ededb64aefd1b83c910551f6f42f7a977b33c3d9209faf2780e79abf4eed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [llama-cpp-python]\n",
            "\u001b[1A\u001b[2KSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZQWB2bPYWco",
        "outputId": "d27ca3fe-a8c7-4b61-80b5-1679f08bd117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "WyVwfgysbke2",
        "outputId": "72070b24-d089-4725-ab4b-41c74b5d4ae9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d10bfeb35272a005cf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d10bfeb35272a005cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d10bfeb35272a005cf.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "import gradio as gr\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"/content/drive/MyDrive/model_q4_k_m.gguf\",\n",
        "    n_ctx=2048,\n",
        "    n_threads=4,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "def build_prompt(history, message):\n",
        "    prompt = \"\"\n",
        "    for user, assistant in history:\n",
        "        prompt += f\"User: {user}\\nAssistant: {assistant}\\n\"\n",
        "    prompt += f\"User: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def chat(message, history):\n",
        "    prompt = build_prompt(history, message)\n",
        "\n",
        "    output = llm(\n",
        "        prompt,\n",
        "        max_tokens=200,\n",
        "        stop=[\"\\nUser:\"],   # ← 必须换成 \\nUser: 避免 history 截断\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    response = output[\"choices\"][0][\"text\"].strip()\n",
        "    return response\n",
        "\n",
        "\n",
        "gr.ChatInterface(chat).launch(\n",
        "    share=True,\n",
        "    debug=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XywMIROOhtKt",
        "outputId": "4e74e813-8edf-4beb-e2d2-e0b6d1d11960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "The token `useage` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `useage`\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q huggingface_hub\n",
        "# !huggingface-cli -h\n",
        "!huggingface-cli login\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO4y2G2GjALN",
        "outputId": "2c6ec01a-462e-49c9-cf1d-183a97aeb06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:   1% 22.6M/2.02G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 22.6M/2.02G [00:03<04:46, 6.97MB/s, 8.09MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   2% 44.9M/2.02G [00:03<02:08, 15.4MB/s, 15.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   3% 67.2M/2.02G [00:03<01:16, 25.4MB/s, 21.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   4% 81.3M/2.02G [00:03<01:02, 31.1MB/s, 23.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   4% 88.6M/2.02G [00:04<01:00, 32.0MB/s, 24.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   5% 103M/2.02G [00:04<00:48, 39.4MB/s, 27.1MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)      :   6% 118M/2.02G [00:04<00:40, 46.9MB/s, 29.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   7% 133M/2.02G [00:04<00:35, 52.7MB/s, 31.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   7% 149M/2.02G [00:04<00:31, 59.3MB/s, 33.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   8% 170M/2.02G [00:05<00:25, 71.2MB/s, 36.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   9% 177M/2.02G [00:05<00:29, 61.9MB/s, 36.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  10% 192M/2.02G [00:05<00:27, 66.1MB/s, 38.5MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  10% 208M/2.02G [00:05<00:26, 69.0MB/s, 39.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  11% 215M/2.02G [00:05<00:30, 59.8MB/s, 39.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  11% 230M/2.02G [00:06<00:27, 64.7MB/s, 41.2MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  12% 246M/2.02G [00:06<00:26, 68.2MB/s, 42.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  13% 254M/2.02G [00:06<00:29, 59.5MB/s, 42.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  13% 269M/2.02G [00:06<00:26, 65.3MB/s, 43.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 293M/2.02G [00:06<00:21, 80.9MB/s, 45.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 296M/2.02G [00:07<00:27, 61.7MB/s, 44.9MB/s  ]\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  15% 296M/2.02G [00:04<00:25, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  15% 296M/2.02G [00:04<00:26, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  15% 296M/2.02G [00:04<00:27, 62.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 297M/2.02G [00:07<01:14, 23.2MB/s, 40.1MB/s  ]\n",
            "New Data Upload               :   1% 529k/67.1M [00:07<16:28, 67.4kB/s, 71.4kB/s  ]\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  15% 297M/2.02G [00:04<00:30, 57.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  15% 297M/2.02G [00:05<00:31, 54.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  15% 297M/2.02G [00:05<00:32, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 301M/2.02G [00:08<01:56, 14.7MB/s, 36.7MB/s  ]\n",
            "New Data Upload               :   4% 4.76M/134M [00:08<02:57, 731kB/s,  580kB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 301M/2.02G [00:08<02:12, 13.0MB/s, 35.9MB/s  ]\n",
            "New Data Upload               :   4% 5.28M/134M [00:08<02:38, 813kB/s,  629kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 303M/2.02G [00:09<02:21, 12.1MB/s, 35.2MB/s  ]\n",
            "New Data Upload               :   5% 6.87M/134M [00:09<01:43, 1.23MB/s,  799kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 306M/2.02G [00:09<02:13, 12.8MB/s, 34.8MB/s  ]\n",
            "New Data Upload               :   7% 10.0M/134M [00:09<00:52, 2.36MB/s, 1.14MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  15% 309M/2.02G [00:09<02:06, 13.5MB/s, 34.4MB/s  ]\n",
            "New Data Upload               :  10% 13.2M/134M [00:09<00:32, 3.69MB/s, 1.47MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  16% 314M/2.02G [00:09<01:46, 15.9MB/s, 34.1MB/s  ]\n",
            "New Data Upload               :  13% 18.0M/134M [00:09<00:18, 6.18MB/s, 1.95MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  16% 321M/2.02G [00:09<01:22, 20.6MB/s, 34.1MB/s  ]\n",
            "New Data Upload               :  18% 24.8M/134M [00:09<00:10, 10.4MB/s, 2.64MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  16% 325M/2.02G [00:10<01:24, 20.0MB/s, 33.8MB/s  ]\n",
            "New Data Upload               :  21% 28.5M/134M [00:10<00:08, 11.8MB/s, 2.97MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  16% 329M/2.02G [00:10<01:20, 21.1MB/s, 33.6MB/s  ]\n",
            "New Data Upload               :  25% 33.2M/134M [00:10<00:07, 14.2MB/s, 3.39MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  17% 337M/2.02G [00:10<01:05, 25.5MB/s, 33.7MB/s  ]\n",
            "New Data Upload               :  30% 40.6M/134M [00:10<00:04, 19.3MB/s, 4.06MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  17% 344M/2.02G [00:10<00:59, 28.0MB/s, 33.7MB/s  ]\n",
            "New Data Upload               :  35% 47.5M/134M [00:10<00:03, 22.9MB/s, 4.65MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  18% 367M/2.02G [00:10<00:30, 54.2MB/s, 36.0MB/s  ]\n",
            "New Data Upload               :  40% 54.3M/134M [00:10<00:03, 25.8MB/s, 5.32MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  19% 384M/2.02G [00:11<00:26, 62.8MB/s, 37.6MB/s  ]\n",
            "New Data Upload               :  47% 62.7M/134M [00:11<00:02, 30.1MB/s, 6.15MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  20% 409M/2.02G [00:11<00:19, 80.8MB/s, 40.1MB/s  ]\n",
            "New Data Upload               :  53% 70.6M/134M [00:11<00:01, 32.8MB/s, 6.92MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  21% 433M/2.02G [00:11<00:16, 93.5MB/s, 42.5MB/s  ]\n",
            "New Data Upload               :  59% 78.5M/134M [00:11<00:01, 34.7MB/s, 7.70MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  23% 458M/2.02G [00:11<00:15, 102MB/s, 44.9MB/s  ] \n",
            "New Data Upload               :  64% 86.4M/134M [00:11<00:01, 36.0MB/s, 8.47MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  24% 475M/2.02G [00:11<00:15, 97.1MB/s, 46.6MB/s  ]\n",
            "New Data Upload               :  71% 94.8M/134M [00:11<00:01, 37.9MB/s, 9.30MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  25% 500M/2.02G [00:12<00:14, 105MB/s, 49.0MB/s  ] \n",
            "New Data Upload               :  77% 103M/134M [00:12<00:00, 38.3MB/s, 10.1MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  25% 514M/2.02G [00:12<00:15, 95.6MB/s, 50.4MB/s  ]\n",
            "New Data Upload               :  81% 109M/134M [00:12<00:00, 36.4MB/s, 10.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  27% 536M/2.02G [00:12<00:14, 99.7MB/s, 52.6MB/s  ]\n",
            "New Data Upload               :  85% 114M/134M [00:12<00:00, 33.2MB/s, 11.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  27% 548M/2.02G [00:12<00:16, 87.8MB/s, 53.8MB/s  ]\n",
            "New Data Upload               :  88% 118M/134M [00:12<00:00, 28.8MB/s, 11.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  28% 561M/2.02G [00:12<00:18, 80.7MB/s, 55.0MB/s  ]\n",
            "New Data Upload               :  91% 122M/134M [00:12<00:00, 26.6MB/s, 12.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  29% 582M/2.02G [00:13<00:16, 87.9MB/s, 57.0MB/s  ]\n",
            "New Data Upload               :  94% 126M/134M [00:13<00:00, 24.9MB/s, 12.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  29% 595M/2.02G [00:13<00:17, 80.2MB/s, 58.3MB/s  ]\n",
            "New Data Upload               :  97% 131M/134M [00:13<00:00, 23.6MB/s, 12.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  30% 614M/2.02G [00:13<00:16, 85.5MB/s, 58.0MB/s  ]\n",
            "New Data Upload               : 100% 134M/134M [00:13<00:00, 21.1MB/s, 13.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  31% 623M/2.02G [00:13<00:19, 73.0MB/s, 56.7MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  32% 640M/2.02G [00:13<00:18, 76.2MB/s, 56.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  32% 648M/2.02G [00:14<00:20, 66.2MB/s, 55.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  33% 665M/2.02G [00:14<00:19, 71.2MB/s, 56.5MB/s  ]\n",
            "New Data Upload               : 100% 134M/134M [00:14<00:00, 8.19MB/s, 13.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  34% 682M/2.02G [00:14<00:17, 75.9MB/s, 56.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  34% 690M/2.02G [00:14<00:20, 65.4MB/s, 56.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  35% 699M/2.02G [00:14<00:22, 58.6MB/s, 55.5MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  35% 716M/2.02G [00:15<00:19, 66.1MB/s, 55.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  36% 724M/2.02G [00:15<00:21, 59.0MB/s, 54.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  36% 732M/2.02G [00:15<00:23, 53.9MB/s, 54.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  37% 741M/2.02G [00:15<00:25, 50.1MB/s, 53.7MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  38% 757M/2.02G [00:15<00:20, 60.2MB/s, 53.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  38% 766M/2.02G [00:16<00:22, 54.9MB/s, 54.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  38% 774M/2.02G [00:16<00:24, 50.9MB/s, 53.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  39% 791M/2.02G [00:16<00:20, 60.8MB/s, 53.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  40% 799M/2.02G [00:16<00:22, 54.9MB/s, 53.5MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  40% 808M/2.02G [00:16<00:23, 51.1MB/s, 52.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  41% 825M/2.02G [00:17<00:19, 61.3MB/s, 52.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  42% 841M/2.02G [00:17<00:17, 68.0MB/s, 53.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  43% 867M/2.02G [00:17<00:13, 85.4MB/s, 55.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  44% 883M/2.02G [00:17<00:13, 84.9MB/s, 57.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  44% 892M/2.02G [00:17<00:15, 72.1MB/s, 58.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  45% 908M/2.02G [00:18<00:14, 75.5MB/s, 60.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  46% 925M/2.02G [00:18<00:14, 78.0MB/s, 61.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  47% 942M/2.02G [00:18<00:13, 79.9MB/s, 63.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  48% 967M/2.02G [00:18<00:11, 93.5MB/s, 65.7MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  49% 984M/2.02G [00:18<00:11, 90.6MB/s, 67.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  50% 1.01G/2.02G [00:19<00:09, 101MB/s, 69.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  51% 1.03G/2.02G [00:19<00:09, 109MB/s, 71.7MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  52% 1.05G/2.02G [00:19<00:09, 101MB/s, 73.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  53% 1.08G/2.02G [00:19<00:08, 109MB/s, 75.2MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  55% 1.10G/2.02G [00:19<00:08, 114MB/s, 77.2MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  55% 1.12G/2.02G [00:20<00:08, 105MB/s, 78.2MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  57% 1.14G/2.02G [00:20<00:07, 111MB/s, 80.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  58% 1.17G/2.02G [00:20<00:07, 116MB/s, 82.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  59% 1.19G/2.02G [00:20<00:06, 118MB/s, 84.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  60% 1.22G/2.02G [00:20<00:06, 121MB/s, 85.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  61% 1.24G/2.02G [00:21<00:07, 110MB/s, 85.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  62% 1.26G/2.02G [00:21<00:06, 114MB/s, 85.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  64% 1.29G/2.02G [00:21<00:06, 118MB/s, 86.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  65% 1.31G/2.02G [00:21<00:05, 120MB/s, 86.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  66% 1.34G/2.02G [00:21<00:05, 122MB/s, 86.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  67% 1.35G/2.02G [00:22<00:06, 111MB/s, 86.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  68% 1.38G/2.02G [00:22<00:05, 115MB/s, 86.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  69% 1.40G/2.02G [00:22<00:05, 118MB/s, 87.2MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  71% 1.43G/2.02G [00:22<00:04, 121MB/s, 87.5MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  72% 1.45G/2.02G [00:22<00:04, 122MB/s, 88.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  73% 1.48G/2.02G [00:23<00:04, 123MB/s, 90.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  74% 1.50G/2.02G [00:23<00:04, 111MB/s, 89.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  75% 1.52G/2.02G [00:23<00:04, 116MB/s, 90.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  77% 1.55G/2.02G [00:23<00:03, 119MB/s, 91.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  78% 1.57G/2.02G [00:23<00:03, 121MB/s, 93.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  79% 1.60G/2.02G [00:24<00:03, 122MB/s, 93.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  80% 1.62G/2.02G [00:24<00:03, 123MB/s, 95.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  82% 1.65G/2.02G [00:24<00:03, 124MB/s, 96.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  83% 1.67G/2.02G [00:24<00:02, 125MB/s, 97.0MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  84% 1.70G/2.02G [00:24<00:02, 125MB/s, 98.7MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  85% 1.72G/2.02G [00:25<00:02, 125MB/s,  100MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  87% 1.75G/2.02G [00:25<00:02, 125MB/s,  101MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  88% 1.77G/2.02G [00:25<00:01, 126MB/s,  103MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  89% 1.80G/2.02G [00:25<00:01, 126MB/s,  104MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  90% 1.82G/2.02G [00:25<00:01, 126MB/s,  106MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  91% 1.84G/2.02G [00:26<00:01, 113MB/s,  106MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  92% 1.86G/2.02G [00:26<00:01, 117MB/s,  108MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  94% 1.89G/2.02G [00:26<00:01, 120MB/s,  109MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  95% 1.92G/2.02G [00:26<00:00, 121MB/s,  110MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  96% 1.94G/2.02G [00:26<00:00, 123MB/s,  112MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  97% 1.96G/2.02G [00:27<00:00, 120MB/s,  113MB/s  ]\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  97% 1.96G/2.02G [00:24<00:00, 80.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  97% 1.97G/2.02G [00:27<00:00, 69.1MB/s,  110MB/s  ]\n",
            "New Data Upload               :  72% 138M/190M [00:27<01:07, 778kB/s,  360kB/s  ] \u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  97% 1.97G/2.02G [00:24<00:00, 79.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  97% 1.97G/2.02G [00:24<00:00, 79.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  97% 1.97G/2.02G [00:24<00:00, 78.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  97% 1.97G/2.02G [00:24<00:00, 77.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  97% 1.97G/2.02G [00:25<00:00, 77.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf:  97% 1.97G/2.02G [00:25<00:00, 76.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 1.97G/2.02G [00:28<00:02, 21.9MB/s, 98.2MB/s  ]\n",
            "New Data Upload               :  74% 140M/190M [00:28<00:58, 868kB/s,  565kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 1.97G/2.02G [00:29<00:02, 20.2MB/s, 96.7MB/s  ]\n",
            "New Data Upload               :  74% 141M/190M [00:29<00:46, 1.04MB/s,  720kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 1.97G/2.02G [00:29<00:02, 19.5MB/s, 94.6MB/s  ]\n",
            "New Data Upload               :  76% 145M/190M [00:29<00:29, 1.54MB/s, 1.03MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 1.98G/2.02G [00:29<00:02, 19.3MB/s, 92.5MB/s  ]\n",
            "New Data Upload               :  78% 148M/190M [00:29<00:18, 2.32MB/s, 1.39MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 1.98G/2.02G [00:29<00:02, 18.6MB/s, 91.1MB/s  ]\n",
            "New Data Upload               :  80% 151M/190M [00:29<00:12, 3.16MB/s, 1.70MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 1.98G/2.02G [00:29<00:01, 17.9MB/s, 89.0MB/s  ]\n",
            "New Data Upload               :  81% 155M/190M [00:29<00:08, 4.19MB/s, 2.00MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 1.99G/2.02G [00:30<00:01, 17.4MB/s, 86.8MB/s  ]\n",
            "New Data Upload               :  83% 158M/190M [00:30<00:06, 5.39MB/s, 2.31MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  99% 1.99G/2.02G [00:30<00:01, 17.0MB/s, 85.5MB/s  ]\n",
            "New Data Upload               :  85% 161M/190M [00:30<00:04, 6.74MB/s, 2.62MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  99% 1.99G/2.02G [00:30<00:01, 18.1MB/s, 83.4MB/s  ]\n",
            "New Data Upload               :  87% 165M/190M [00:30<00:02, 8.97MB/s, 3.03MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  99% 2.00G/2.02G [00:30<00:01, 18.9MB/s, 81.4MB/s  ]\n",
            "New Data Upload               :  89% 169M/190M [00:30<00:01, 11.2MB/s, 3.44MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  99% 2.00G/2.02G [00:30<00:00, 18.0MB/s, 79.2MB/s  ]\n",
            "New Data Upload               :  91% 172M/190M [00:30<00:01, 12.1MB/s, 3.75MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  99% 2.01G/2.02G [00:31<00:00, 19.6MB/s, 77.2MB/s  ]\n",
            "New Data Upload               :  93% 177M/190M [00:31<00:00, 14.7MB/s, 4.22MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 2.01G/2.02G [00:31<00:00, 18.5MB/s, 75.9MB/s  ]\n",
            "New Data Upload               :  95% 180M/190M [00:31<00:00, 15.0MB/s, 4.52MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 2.01G/2.02G [00:31<00:00, 17.7MB/s, 73.7MB/s  ]\n",
            "New Data Upload               :  96% 183M/190M [00:31<00:00, 15.2MB/s, 4.83MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 2.02G/2.02G [00:31<00:00, 17.1MB/s, 71.5MB/s  ]\n",
            "New Data Upload               :  98% 187M/190M [00:31<00:00, 15.3MB/s, 5.14MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 2.02G/2.02G [00:31<00:00, 16.7MB/s, 69.4MB/s  ]\n",
            "New Data Upload               : 100% 190M/190M [00:31<00:00, 15.4MB/s, 5.45MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 2.02G/2.02G [00:32<00:00, 12.5MB/s, 67.0MB/s  ]\n",
            "New Data Upload               : 100% 190M/190M [00:32<00:00, 11.8MB/s, 5.50MB/s  ]\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:29<00:00, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:29<00:00, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:29<00:00, 67.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:29<00:00, 67.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 2.02G/2.02G [00:33<00:00, 4.04MB/s, 55.5MB/s  ]\n",
            "New Data Upload               : 100% 190M/190M [00:33<00:00, 3.93MB/s, 5.51MB/s  ]\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:29<00:00, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:30<00:00, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:30<00:00, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 2.02G/2.02G [00:33<00:00, 60.0MB/s, 47.3MB/s  ]\n",
            "New Data Upload               : 100% 190M/190M [00:33<00:00, 5.66MB/s, 5.62MB/s  ]\n",
            "  ...MyDrive/model_q4_k_m.gguf: 100% 2.02G/2.02G [00:30<00:00, 65.7MB/s]\n",
            "https://huggingface.co/stevendhasoi/Iriseder/blob/main/model_q4_k_m.gguf\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli upload \\\n",
        "  stevendhasoi/Iriseder \\\n",
        "  /content/drive/MyDrive/model_q4_k_m.gguf \\\n",
        "  --repo-type=model\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}